
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   DEEP LEARNING LAB - COMPLETE PACKAGE                    â•‘
â•‘                    All 3 Tasks with Code & Report Template                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PACKAGE CONTENTS:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“‚ PYTHON CODE (Ready to Run)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ“ task1_mnist.py              â†’ MNIST Learning Rate Search
âœ“ task2_fashion_mnist.py      â†’ Fashion-MNIST Activation Analysis  
âœ“ task3_cifar10.py            â†’ CIFAR-10 Optimizers & Batch Norm

ğŸ“‚ DOCUMENTATION (Complete Guides)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ“ lab_report_template.md      â†’ Professional lab report (fill in your values)
âœ“ README.md                   â†’ Detailed instructions for all tasks
âœ“ QUICK_REFERENCE.md          â†’ Quick cheat sheet with key info
âœ“ INSTALLATION.md             â†’ Setup guide for all platforms
âœ“ SUMMARY.txt                 â†’ This file

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
EXECUTION TIMELINE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Task 1: MNIST Learning Rate Search
  â”œâ”€ Runtime: ~10 minutes
  â”œâ”€ Outputs: 2 plots
  â””â”€ Final Metric: Test Accuracy >98%

Task 2: Fashion-MNIST 100 Layers
  â”œâ”€ Runtime: ~15 minutes
  â”œâ”€ Outputs: 3 plots
  â””â”€ Final Metric: Activation accuracy comparison

Task 3: CIFAR-10 Optimizers
  â”œâ”€ Runtime: ~30-45 minutes
  â”œâ”€ Outputs: 4 plots
  â””â”€ Final Metric: 7 optimizer comparison

TOTAL TIME: 1.5-2 hours (all tasks)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
WHAT EACH TASK TEACHES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

TASK 1: LEARNING RATE OPTIMIZATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Key Concept: Finding optimal learning rate through exponential search

What You'll Learn:
  â€¢ How learning rate affects convergence speed and stability
  â€¢ Vanishing learning rate (too small â†’ no updates)
  â€¢ Exploding learning rate (too large â†’ divergence)
  â€¢ Batch normalization for training stability
  â€¢ Training curves interpretation

Expected Results:
  âœ“ Optimal LR identified: 0.001 - 0.01
  âœ“ Test accuracy: >98% (often 99%+)
  âœ“ Training time: ~10 minutes
  âœ“ Generates: Learning rate search plot + training curves

Key Insight:
  Learning rate is THE most important hyperparameter. Small changes (2-3 
  orders of magnitude) completely change training behavior.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

TASK 2: ACTIVATION FUNCTIONS & VANISHING GRADIENTS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Key Concept: Why sigmoid fails in deep networks but SELU succeeds

Architecture: 100 layers Ã— 100 neurons (intentionally very deep!)

What You'll Learn:
  â€¢ Vanishing gradient problem: gradients â†’ 0 in early layers
  â€¢ Exploding gradient problem: gradients â†’ âˆ
  â€¢ Why sigmoid/tanh are problematic for deep networks
  â€¢ How ReLU mitigates the problem
  â€¢ Why SELU is best for very deep networks

Expected Results:
  âœ“ Sigmoid (~50% acc): Severe vanishing gradients
  âœ“ ReLU (~85% acc): Better but imperfect
  âœ“ ELU (~90% acc): Good gradient flow
  âœ“ SELU (~93% acc): Best for 100+ layers

Key Insight:
  Different activations cause gradients to change differently:
  - Sigmoid: Gradient multiplies by max 0.25
  - ReLU: Gradient = 1 (but dead neurons)
  - SELU: Self-normalizing (maintains gradient scale)

Math: With 100 layers of sigmoid:
  Gradient = 0.25^100 â‰ˆ 10^-60 (essentially zero!)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

TASK 3: BATCH NORMALIZATION & OPTIMIZER COMPARISON
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Key Concepts: 
  A) Batch normalization speeds up training
  B) Different optimizers have very different convergence speeds

Part A - Batch Normalization:
  â”œâ”€ Model 1: 20-layer DNN WITHOUT batch norm â†’ baseline
  â”œâ”€ Model 2: 20-layer DNN WITH batch norm â†’ improved
  â””â”€ Compare: Convergence speed, accuracy, stability

  Expected Impact:
  âœ“ Faster: ~20-30% fewer epochs needed
  âœ“ Better: ~3-5% accuracy improvement
  âœ“ Smoother: More stable loss curves

  Why it works:
  â€¢ Normalizes layer inputs: (x - Î¼) / Ïƒ
  â€¢ Maintains mean â‰ˆ 0, variance â‰ˆ 1
  â€¢ Enables larger learning rates
  â€¢ Acts as regularizer

Part B - Optimizer Comparison:
  â”œâ”€ SGD: Vanilla gradient descent (baseline)
  â”œâ”€ Momentum: Accumulates gradient direction
  â”œâ”€ Nesterov: Momentum with "look-ahead"
  â”œâ”€ AdaGrad: Adaptive per-parameter learning rate
  â”œâ”€ RMSProp: Improved AdaGrad
  â”œâ”€ Adam: Combines momentum + adaptive (industry standard)
  â””â”€ Nadam: Adam + Nesterov (best modern option)

  Expected Ranking (Convergence Speed):
  1st: Nadam â‰ˆ Adam (~15 epochs)
  2nd: RMSProp (20 epochs)
  3rd: Nesterov â‰ˆ Momentum (~25 epochs)
  4th: SGD (30+ epochs)
  Last: AdaGrad (~35+ epochs)

  Why differences exist:
  â€¢ SGD: Simple but slow, easily gets stuck
  â€¢ Momentum: Accumulates velocity, escapes plateaus
  â€¢ Nesterov: "Look before you leap" strategy
  â€¢ Adaptive: Adjust learning rate per parameter
  â€¢ Adam/Nadam: Best of all worlds

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
QUICK START GUIDE (5 MINUTES)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. INSTALL (1 minute)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   pip install tensorflow keras numpy matplotlib

2. RUN TASKS (120 minutes)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   python task1_mnist.py
   python task2_fashion_mnist.py
   python task3_cifar10.py

   (Or run individually - each is standalone)

3. COLLECT RESULTS (5 minutes)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Console output + Generated PNG files

4. FILL REPORT (15 minutes)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Open lab_report_template.md
   Replace [Your value] with actual results
   Copy-paste console output
   Insert PNG plots

5. GENERATE PDF (2 minutes)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   pandoc lab_report_template.md -o lab_report.pdf

   (Or use Word/Google Docs to manually format)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
GENERATED PLOTS (Automatically Created During Execution)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Task 1 Output:
  ğŸ“Š task1_lr_search.png
     â””â”€ Shows validation loss for each learning rate
     â””â”€ Identifies optimal LR as minimum point

  ğŸ“Š task1_training_curves.png
     â”œâ”€ Left: Training vs Validation Accuracy
     â””â”€ Right: Training vs Validation Loss

Task 2 Output:
  ğŸ“Š task2_activation_comparison.png
     â””â”€ 4 subplots: sigmoid, relu, elu, selu accuracy curves

  ğŸ“Š task2_loss_comparison.png
     â””â”€ 4 subplots: sigmoid, relu, elu, selu loss curves

  ğŸ“Š task2_accuracy_summary.png
     â””â”€ Bar chart: Final test accuracy for each activation

Task 3 Output:
  ğŸ“Š task3_lr_search.png
     â””â”€ Learning rate search for CIFAR-10

  ğŸ“Š task3_batch_norm_comparison.png
     â”œâ”€ Left: Accuracy with/without BN
     â””â”€ Right: Loss with/without BN

  ğŸ“Š task3_optimizer_comparison.png
     â”œâ”€ Top-left: Training accuracy by optimizer
     â”œâ”€ Top-right: Validation accuracy
     â”œâ”€ Bottom-left: Training loss
     â””â”€ Bottom-right: Validation loss

  ğŸ“Š task3_optimizer_summary.png
     â”œâ”€ Left: Final accuracy by optimizer
     â””â”€ Right: Final loss by optimizer

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
REPORT STRUCTURE (What to Include)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Your final report should have:

1. TITLE PAGE
   â”œâ”€ Title: "Deep Learning Lab - Tasks 1-3"
   â”œâ”€ Your name
   â”œâ”€ Date
   â””â”€ Course name

2. TASK 1 SECTION (~2 pages)
   â”œâ”€ Objective
   â”œâ”€ Methodology (architecture, training setup)
   â”œâ”€ Results (optimal LR, final accuracy)
   â”œâ”€ task1_lr_search.png (with caption)
   â”œâ”€ task1_training_curves.png (with caption)
   â””â”€ Analysis: Why this LR is optimal

3. TASK 2 SECTION (~3 pages)
   â”œâ”€ Objective
   â”œâ”€ Methodology (100 layers, activation functions)
   â”œâ”€ Vanishing Gradient Explanation
   â”œâ”€ task2_activation_comparison.png
   â”œâ”€ task2_loss_comparison.png
   â”œâ”€ task2_accuracy_summary.png
   â””â”€ Analysis: Why SELU > ELU > ReLU > Sigmoid

4. TASK 3 SECTION (~4 pages)
   â”œâ”€ Objective
   â”œâ”€ Part A: Batch Normalization
   â”‚  â”œâ”€ task3_batch_norm_comparison.png
   â”‚  â””â”€ Analysis: Speed-up and accuracy gain
   â”œâ”€ Part B: Optimizer Comparison
   â”‚  â”œâ”€ task3_optimizer_comparison.png
   â”‚  â”œâ”€ task3_optimizer_summary.png
   â”‚  â””â”€ Analysis: Convergence speed differences
   â””â”€ Optimizer descriptions (SGD through Nadam)

5. CONCLUSION (~1 page)
   â”œâ”€ Summary of findings
   â”œâ”€ Key learnings
   â””â”€ When to use what

Total: ~10-12 pages (professional, not too long)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
CODE EXECUTION CHECKLIST
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

BEFORE RUNNING:
â˜ Python 3.8+ installed
â˜ pip install tensorflow keras numpy matplotlib
â˜ CUDA/cuDNN installed (optional but recommended for Task 3)
â˜ 4+ GB RAM available
â˜ 2+ GB disk space free

RUNNING TASK 1:
â˜ python task1_mnist.py executes
â˜ Console shows "Learning Rate Search"
â˜ task1_lr_search.png created
â˜ Console shows "Training with Optimal LR"
â˜ Epoch training output visible
â˜ Final Test Accuracy printed
â˜ task1_training_curves.png created

RUNNING TASK 2:
â˜ python task2_fashion_mnist.py executes
â˜ Loads Fashion MNIST dataset
â˜ Tests 4 activation functions
â˜ task2_activation_comparison.png created
â˜ task2_loss_comparison.png created
â˜ task2_accuracy_summary.png created
â˜ Console shows final accuracies

RUNNING TASK 3:
â˜ python task3_cifar10.py executes
â˜ Learning rate search (5 epochs each)
â˜ Training without BN (30 epochs)
â˜ Training with BN (30 epochs)
â˜ task3_batch_norm_comparison.png created
â˜ 7 optimizers train sequentially
â˜ task3_optimizer_comparison.png created
â˜ task3_optimizer_summary.png created

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PRESENTATION TIPS FOR LAB SESSION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

WHAT TO EMPHASIZE:

Task 1 Demo:
  â€¢ Show the learning rate search plot
  â€¢ Point out: "This LR converges fastest"
  â€¢ Explain: "If LR was 10x smaller, training would take forever"
  â€¢ Explain: "If LR was 10x larger, loss would explode"
  â€¢ Conclusion: "Finding optimal LR is crucial"

Task 2 Demo:
  â€¢ Show SELU vs Sigmoid plots side-by-side
  â€¢ Point: "Look at sigmoid - it never improves!"
  â€¢ Explain: "With 100 layers, gradients vanish"
  â€¢ Show math: "0.25^100 â‰ˆ 10^-60"
  â€¢ Conclusion: "Activation function determines depth capability"

Task 3 Demo - Batch Norm:
  â€¢ Show improvement from BN
  â€¢ Metric: "Convergence speedup: X%"
  â€¢ Metric: "Accuracy improvement: X%"
  â€¢ Conclusion: "BN is essential for modern deep learning"

Task 3 Demo - Optimizers:
  â€¢ Show convergence speed ranking
  â€¢ Emphasize: "Adam/Nadam are 2-3x faster than SGD"
  â€¢ Show: "Why Adam dominates industry"
  â€¢ Explain: "Adam combines momentum + per-param learning rates"

QUESTIONS YOU SHOULD PREPARE FOR:

Q1: Why does sigmoid fail with 100 layers?
A: Gradients multiply through layers. Sigmoid derivative max = 0.25.
   0.25^100 â‰ˆ 10^-60 â†’ effectively zero â†’ no learning in early layers.

Q2: What's batch normalization really doing?
A: Normalizing layer inputs to mean=0, variance=1. Prevents internal
   covariate shift, maintains gradient flow, enables larger learning rates.

Q3: When would you NOT use Adam?
A: Fine-tuning pretrained models (SGD+momentum), very specific research
   cases, or when memory is extremely limited. But Adam is the default.

Q4: Can you train 200-layer network without modern activations?
A: No. Sigmoid/tanh have vanishing gradient problems. SELU/batch norm
   enable networks with 100+ layers. Without them, impossible.

Q5: Which optimizer converges fastest?
A: Nadam â‰ˆ Adam. But difference matters mostly in big models.
   For small models, any modern optimizer works fine.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TROUBLESHOOTING DURING EXECUTION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Issue: "No module named 'tensorflow'"
Solution: pip install tensorflow --upgrade

Issue: Training very slow
Reason: GPU not detected (CPU fallback)
Solution: Install CUDA/cuDNN or accept slower training

Issue: "Out of memory" error
Solution: Reduce batch_size in code (128 â†’ 64)

Issue: Plots not showing
Solution: They auto-save as PNG files (don't need to display)

Issue: Task 3 takes too long
Solution: Reduce epochs in code (30 â†’ 10 for quick test)

Issue: Different results than expected
Normal: Stochasticity means results vary slightly. Same trends appear.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
KEY FORMULAS FOR REPORT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SIGMOID VANISHING GRADIENTS:
  Ïƒ(x) = 1 / (1 + e^-x)
  Ïƒ'(x) = Ïƒ(x) * (1 - Ïƒ(x))  [max value: 0.25 at x=0]
  Chain rule: dL/dwâ‚ = dL/doutput Ã— doutput/dhâ‚ Ã— ... Ã— dhâ‚/dwâ‚
  With 100 sigmoid layers: gradient â‰ˆ 0.25^100 â‰ˆ 10^-60

RELU GRADIENTS:
  ReLU(x) = max(0, x)
  ReLU'(x) = 1 if x > 0, else 0
  No multiplication: gradient = 1 (but dead neurons issue)

SELU SELF-NORMALIZATION:
  After each layer, mean and variance maintained at:
  E[x] = 0, Var[x] = 1
  Enables training 100+ layers naturally

BATCH NORMALIZATION:
  For layer inputs x in mini-batch:
  xÌ‚ = (x - Î¼_batch) / âˆš(ÏƒÂ²_batch + Îµ)  [normalize]
  y = Î³ * xÌ‚ + Î²                         [scale]
  Result: Mean = 0, Variance = 1

ADAM OPTIMIZER:
  m_t = Î²â‚*m_{t-1} + (1-Î²â‚)*g_t         [momentum: 1st moment]
  v_t = Î²â‚‚*v_{t-1} + (1-Î²â‚‚)*g_tÂ²       [RMSProp: 2nd moment]
  w_t = w_t - Î± * m_t / (âˆšv_t + Îµ)     [adaptive update]

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
EXPECTED RESULTS SUMMARY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

TASK 1 (MNIST):
  Expected Test Accuracy: 98-99%+
  Expected Optimal LR: 0.001 - 0.01
  Training Time: ~10 minutes
  Key Result: >98% accuracy achieved âœ“

TASK 2 (Fashion-MNIST):
  Sigmoid Accuracy: ~50% (fails badly)
  ReLU Accuracy: ~83-87% (works okay)
  ELU Accuracy: ~88-92% (works well)
  SELU Accuracy: ~90-94% (best)
  Key Result: Clear evidence of vanishing gradient problem âœ“

TASK 3 (CIFAR-10):
  Without BN Accuracy: ~45-50%
  With BN Accuracy: ~50-55% (5-10% improvement!)
  Best Optimizer: Nadam or Adam (fastest convergence)
  Worst Optimizer: SGD (slowest, ~3x slower)
  Key Result: BN improves convergence, modern optimizers essential âœ“

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
FILE MANIFEST
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“„ CODE FILES (Run These)
  âœ“ task1_mnist.py (394 lines)
    â””â”€ Executable: python task1_mnist.py

  âœ“ task2_fashion_mnist.py (327 lines)
    â””â”€ Executable: python task2_fashion_mnist.py

  âœ“ task3_cifar10.py (412 lines)
    â””â”€ Executable: python task3_cifar10.py

ğŸ“‹ DOCUMENTATION (Read These)
  âœ“ lab_report_template.md
    â””â”€ Professional report template - fill in your results

  âœ“ README.md
    â””â”€ Comprehensive guide with all instructions

  âœ“ QUICK_REFERENCE.md
    â””â”€ Cheat sheet with key concepts

  âœ“ INSTALLATION.md
    â””â”€ Setup guide for all platforms

  âœ“ SUMMARY.txt
    â””â”€ This file - complete overview

ğŸ–¼ï¸ GENERATED PLOTS (Appear After Running)
  task1_lr_search.png
  task1_training_curves.png
  task2_activation_comparison.png
  task2_loss_comparison.png
  task2_accuracy_summary.png
  task3_lr_search.png
  task3_batch_norm_comparison.png
  task3_optimizer_comparison.png
  task3_optimizer_summary.png

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
FINAL CHECKLIST FOR SUBMISSION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CODE SUBMISSION:
â˜ task1_mnist.py (working, tested)
â˜ task2_fashion_mnist.py (working, tested)
â˜ task3_cifar10.py (working, tested)
â˜ All .py files are executable
â˜ All files have descriptive comments

REPORT SUBMISSION:
â˜ lab_report.pdf created (from template)
â˜ All console outputs captured
â˜ All 9 plots included with captions
â˜ All [Your value] placeholders filled
â˜ Analysis sections completed (not template text)
â˜ Vanishing gradient section explained
â˜ Batch norm impact analyzed
â˜ Optimizer differences discussed
â˜ Conclusion written

PRESENTATION READINESS:
â˜ Understand why each task matters
â˜ Can explain vanishing gradient problem
â˜ Know what batch normalization does
â˜ Can explain why Adam > SGD
â˜ Comfortable answering Q&A about results
â˜ Can show code and explain key sections
â˜ Ready to discuss results interpretation

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SUCCESS CRITERIA:

âœ“ All 3 tasks execute without errors
âœ“ Expected accuracy/loss ranges met
âœ“ All 9 plots successfully generated
âœ“ Professional lab report submitted (PDF)
âœ“ Code and report presented to teacher
âœ“ Can discuss observations and results

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

You now have EVERYTHING needed to complete this lab successfully!

Questions? Check README.md or QUICK_REFERENCE.md

Good luck! ğŸš€

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
